{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Overview -- Web Scraping\n",
      "\n",
      "We will be using the Wikipedia API to download articles from the web.  Eventually we will be performing NLP and machine learning on the articles to do awesome things like document clustering, document filtering/classification, network analysis, and some indexing.  For this sprint we are just converened with getting some data.\n",
      "\n",
      "## Goals\n",
      "\n",
      "* __Get experience using an [API](http://en.wikipedia.org/wiki/Application_programming_interface) to access [Wikipedia](http://www.wikipedia.org/) articles__\n",
      "* __Store the retrieved articles and metadata in [MongoDB](http://www.mongodb.org/)__ \n",
      "* __Use [regular expressions](http://en.wikipedia.org/wiki/Regular_expression) in [Python](http://docs.python.org/2/howto/regex.html) to search for all articles that contain the word 'Zipf' or 'Zipfian'__\n",
      "* __Augment the article content with contextual information from its [external links](http://en.wikipedia.org/wiki/Wikipedia:External_links)__ \n",
      "* __Have [FUN!](http://media.giphy.com/media/LlmVkDId8FzP2/giphy.gif) (that's an order)__"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Resources\n",
      "\n",
      "* __[Requests HTTP library](http://docs.python-requests.org/en/latest/)__\n",
      "* __[Regular expression tester](http://pythex.org/)__\n",
      "* __[Google Regex tutorial](https://developers.google.com/edu/python/regular-expressions)__\n",
      "* __[Beautiful Soup (HTML parsing and searching)](http://www.crummy.com/software/BeautifulSoup/)__\n",
      "* __[MongoDB Python driver](http://api.mongodb.org/python/current/tutorial.html)__"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data Sources (ranked by ease of use... usually)\n",
      "\n",
      "### DaaS -- Data as a service \n",
      "* time series: [Quandl](http://www.quandl.com/)\n",
      "* public datasets: [enigma](http://enigma.io/)\n",
      "* location contextualization: [factual](http://www.factual.com/)\n",
      "* financial modeling: [Quantopian](https://www.quantopian.com/)\n",
      "* email contextualization: [Rapleaf](http://www.rapleaf.com/why-rapleaf/)\n",
      "* social media: [Gnip](http://gnip.com/)\n",
      "        \n",
      "### Bulk Downloads -- just like the good ol' days\n",
      "\n",
      "* FTP servers\n",
      "* Amazon S3 public [datasets](http://aws.amazon.com/publicdatasets/)\n",
      "* [InfoChimps](http://www.infochimps.com/datasets)\n",
      "* Academia -- [Stanford](http://snap.stanford.edu/data/) and [UCI](http://archive.ics.uci.edu/ml/)\n",
      "\n",
      "### APIs -- public and hidden\n",
      "* [Twitter](https://dev.twitter.com/)\n",
      "* [Foursquare](https://developer.foursquare.com/)\n",
      "* [Facebook](https://developers.facebook.com/docs/reference/apis/)\n",
      "* [Tumblr](http://www.tumblr.com/docs/en/api/v2)\n",
      "* [Rdio](http://developer.rdio.com/)\n",
      "* [Yelp](http://www.yelp.com/developers/documentation)\n",
      "* [Last.fm](http://www.last.fm/api)\n",
      "* [bitly](http://dev.bitly.com/)\n",
      "* [LinkedIn](https://developer.linkedin.com/apis)\n",
      "* [Yahoo Finance (hidden)](http://greenido.wordpress.com/2009/12/22/yahoo-finance-hidden-api/)\n",
      "* [etc.](http://developer.trulia.com/)\n",
      "* [etc.](http://dev.evernote.com/documentation/cloud/)\n",
      "* [etc.](http://www.songkick.com/developer/)\n",
      "\n",
      "### DIY\n",
      "* Webscraping \n",
      "* manual downloads\n",
      "\n",
      "### If you have any other favorite datasources, please post them to [Piazza](https://piazza.com/class/hkpmwswpcjcxq)!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise: Wikipedia++"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import Python's standard library modules for regular expresions and json\n",
      "import re\n",
      "import json\n",
      "\n",
      "# This is useful to fix malformed urls \n",
      "import urlparse\n",
      "\n",
      "# import the Image display module\n",
      "from IPython.display import Image\n",
      "\n",
      "# inline allows us to embed matplotlib figures directly into the IPython notebook\n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING: pylab import has clobbered these variables: ['text', 'title']\n",
        "`%pylab --no-import-all` prevents importing * from pylab and numpy\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step 1: Access the Wikipedia API\n",
      "\n",
      "Lucky for us the Wikipedia [API](http://www.mediawiki.org/wiki/API) is well [documented](http://www.mediawiki.org/wiki/API:FAQ).  And you do not need an API key to access it (isn't that nice of them).  Go ahead, give it a spin!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import the Requests HTTP library\n",
      "import requests\n",
      "\n",
      "# A User agent header required for the Wikipedia API.\n",
      "headers = {'user_agent': 'DataWrangling/1.1 (http://zipfianacademy.com; class@zipfianacademy.com)'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment with fetching one or two pages and examining the result (fill in URL and payload)\n",
      "url = 'http://en.wikipedia.org/w/api.php'\n",
      "\n",
      "# parameters for the API request\n",
      "payload = {'action':'parse', 'page':'Zipf\\'s_law', 'prop':'links', 'format':'json'}\n",
      "\n",
      "# make the request\n",
      "r = requests.post(url, data=payload, headers=headers)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__HINT: Check out the [parse](http://www.mediawiki.org/wiki/API:Parsing_wikitext#parse) action___"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step 2: Persistence in MongoDB\n",
      "\n",
      "Now that you have some experience with the API and can sucessfully access articles with associated metadata, it is time to start storing them in [MongoDB](http://www.mongodb.org/)!\n",
      "\n",
      "You should have a MongoDB [daemon](http://docs.mongodb.org/manual/tutorial/manage-mongodb-processes/) running on your vagrant machine.  It is here that you will be storing all of your data, but be aware of how many articles you are crawling. \n",
      "\n",
      "#### One article = ~120 kilobytes.  500MB / 120KB \u2248 4,250 articles."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import MongoDB modules\n",
      "from pymongo import MongoClient\n",
      "from bson.objectid import ObjectId\n",
      "\n",
      "# connect to the hosted MongoDB instance\n",
      "client = MongoClient('mongodb://localhost:27017/')\n",
      "\n",
      "# connect to the wikipedia database: if it does not exist it will automatically create it -- one reason why mongoDB can be nice.\n",
      "db = client.wikipedia"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 116
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each [database](http://docs.mongodb.org/manual/reference/glossary/#term-database) has a number of [collections](http://docs.mongodb.org/manual/reference/glossary/#term-collection) analogous to SQL tables.  And each collection is comprised of [documents](http://docs.mongodb.org/manual/reference/glossary/#term-document) analogous to a rows in a SQL table.  And each document has [fields](http://docs.mongodb.org/manual/reference/glossary/#term-field) analogous to SQL columns.  Also, the docs have made a more comprehensive [comparision](http://docs.mongodb.org/manual/reference/sql-comparison/).\n",
      "                \n",
      "![mongo_diagram](http://zipfianacademy.com/data/images/mongo_diagram.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that you can store and retrieve articles in the Mongo Database, it is time to iterate!\n",
      "\n",
      "### Step 3: Retrieve and store every article (with associated metadata) within 2 hops from the 'Zipf's law' article.  \n",
      "<b style=\"color: red\">Do not follow external links, only linked Wikipedia articles</b>\n",
      "\n",
      "___HINT: The Zipf's Law article should be located at: 'http://en.wikipedia.org/w/api.php?action=parse&format=json&page=Zipf's%20law'___"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# grab the list of linked Wikipedia articles from the API result \n",
      "\n",
      "links = [item['*'] for item in r.json()['parse']['links']]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# iterate over each link and store the returned document in MongoDB\n",
      "db.wikipedia.remove()\n",
      "\n",
      "i=1\n",
      "for link in links:\n",
      "    if i % 10 == 0:\n",
      "        print i\n",
      "    i += 1\n",
      "    url = 'http://en.wikipedia.org/w/api.php'\n",
      "\n",
      "    # parameters for the API request\n",
      "    payload = {'action':'parse', 'page':link, 'format':'json'}\n",
      "\n",
      "    # make the request\n",
      "    req = requests.post(url, data=payload, headers=headers, allow_redirects = True)\n",
      "    db.wikipedia.insert(req.json())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10\n",
        "20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "60"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "70"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "80"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "90"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "110"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "120"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "130"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "140"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "150"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "160"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "170"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "180"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "190"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "210"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "220"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "230"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "db.wikipedia.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "239"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step 3: Find all articles that mention 'Zipf' or 'Zipfian' (case insensitive)\n",
      "\n",
      "We will get some practice now with regular expressions in order to search the content of the articles for the terms `Zipf` or `Zipfian`.  We only want articles that mention these terms in the displayed text however, so we must first remove all the unnecessary HTML tags and only keep what is in between the relevant tags.  Beautiful Soup makes this almost trivial.  Explore the documentation to find how to do this effortlessly: [http://www.crummy.com/software/BeautifulSoup/bs4/doc/](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)  \n",
      "\n",
      "Test out your Regular Expressions __before__ you run them over __every__ document you have in your database: [http://pythex.org/](http://pythex.org/).  Here is some useful documentation on regular expressions in Python: [http://docs.python.org/2/howto/regex.html](http://docs.python.org/2/howto/regex.html)\n",
      "        \n",
      "Once you have identified the relevant articles, save them to a file for now, we do not need to persist them in the database (but you can if you want)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import the Beautiful Soup module \n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "p = re.compile('zipf',re.IGNORECASE)\n",
      "zipf_titles = []\n",
      "\n",
      "for cursor in db.wikipedia.find():\n",
      "    \n",
      "    html = cursor['parse']['text']['*']\n",
      "\n",
      "    soup = BeautifulSoup(html)\n",
      "    text = soup.getText()\n",
      "    if p.search(text):\n",
      "        zipf_titles.append(cursor['parse']['title'])\n",
      "        \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 118
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(zipf_titles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "180"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step 4: Augmentation!  Time to remix the web... or rather just Wikipedia. But hey, isn't Wikipedia the web.\n",
      "\n",
      "![magritte_remix](http://25.media.tumblr.com/0b608535743de2928ea2c3a76b771fcd/tumblr_mogx75qjxQ1qedb29o1_500.gif)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to augment our Zipfian Wikipedia articles with content from the __WWW__ at large.  Stepping out of the walled garden of collaboratively edited document safety... let us scrape!  For each of the artcles we found to contain __'Zipf'__ or __'Zipfian'__, we want to know what the web has to say.  For each of the ___external links___ of said articles, fetch the linked webpage and extract the __&lt;title&gt;__ and __&lt;meta name=\"keywords\"&gt;__ from the __HTML__.  Beautiful Soup would probably help you a lot here. \n",
      "\n",
      "___You still have to watch out for pages without keywords or a title___\n",
      "\n",
      "Once you have extracted this information, update the stored document in your database with this information. Add a field called __'extraexternal'__ that contains the additional contextual information.  __'extraexternal'__ should be an array of __JSON__ objects, each of which have keys:\n",
      "    \n",
      "* __'url'__ : the url of the page\n",
      "* __'title'__ : the title of the page\n",
      "* __'keywords'__ : the keywords from the meta tag\n",
      "    \n",
      "__Example:__\n",
      "    \n",
      "        {\n",
      "         \n",
      "         ...\n",
      "         \n",
      "         'displaytitle': \"Zipf's law\",\n",
      "         'externallinks': [...],\n",
      "         'text': {\n",
      "                    '*': '<table class=\"infobox bordered\" style=\"width:325px; max-width:325px; font-size:95%; text-align: left;\">\\n<caption>Zipf\\'s law</caption>\\n<tr...'\n",
      "                  }\n",
      "        'extraexternal' : [{ \n",
      "                             'url' : 'http://zipfianacademy.com',\n",
      "                             'title' : 'Teaching the Long Tail | Zipfian Academy'\n",
      "                             'keywords' : 'data, datascience, science, bootcamp, training, hadoop, big, bigdata, boot, camp, machine...'\n",
      "                           }, ... ]\n",
      "         ...\n",
      "        } \n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# re-open our output file of matched articles \n",
      "\n",
      "i=1\n",
      "\n",
      "for title in zipf_titles[:5]:\n",
      "    print i, ':', title\n",
      "    \n",
      "    links = db.wikipedia.find_one({'parse.title':title})['parse']['externallinks']\n",
      "    print len(links), ' external links'\n",
      "    \n",
      "    extraexternal = []\n",
      "    \n",
      "    for link in links[:]:\n",
      "        if 'http' not in link:\n",
      "            link = 'http:'+link\n",
      "        \n",
      "        print link\n",
      "        \n",
      "        try:\n",
      "            r_link = requests.get(link,timeout=1)\n",
      "        \n",
      "        \n",
      "            if 'html' in r_link.headers['content-type']:\n",
      "                html = r_link.content\n",
      "                \n",
      "            link_external = {}\n",
      "            soup = BeautifulSoup(html)\n",
      "            link_external['url'] = r_link.url\n",
      "            \n",
      "            if soup.find('title'):\n",
      "                link_external['title'] = soup.find('title').get_text()\n",
      "                \n",
      "            if soup.select('meta[name=\"keywords\"]'):\n",
      "                link_external['keywords'] = soup.select('meta[name=\"keywords\"]')[0].attrs['content']\n",
      "            else:\n",
      "                link_external['keywords'] = None\n",
      "                \n",
      "            extraexternal.append(link_external)\n",
      "        except:\n",
      "            pass\n",
      "    \n",
      "    objectid = db.wikipedia.find_one({'parse.title':title})['_id']\n",
      "    db.wikipedia.update({'_id':objectid},{'$set':{'parse.cupcake':extraexternal}})\n",
      "        \n",
      "    i+=1  \n",
      "        \n",
      "\n",
      "# iterate over each article that contains 'Zipf' or 'Zipfian'\n",
      "\n",
      "# TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 : Wishart distribution\n",
        "8  external links\n",
        "http://dx.doi.org/10.1093%2Fbiomet%2F20A.1-2.32\n",
        "http://www.zentralblatt-math.org/zmath/en/search/?format=complete&q=an:54.0565.02"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://www.jstor.org/stable/2331939"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176325375"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://www.jstor.org/stable/2346290"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://dx.doi.org/10.1214%2Faop%2F1176990455"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://dx.doi.org/10.1007%2FBF01078179"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "http://www.jstor.org/pss/2283988"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " : Wrapped exponential distribution\n",
        "2  external links\n",
        "http://www.pstat.ucsb.edu/faculty/jammalam/html/Some%20Publications/2004_WrappedSkewFamilies_Comm..pdf\n",
        "http://dx.doi.org/10.1081%2FSTA-200026570"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " : Von Mises\u2013Fisher distribution\n",
        "2  external links\n",
        "http://dx.doi.org/10.1007%2Fs00180-011-0232-x\n",
        "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.186.1887&rep=rep1&type=pdf"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " : Natural exponential family\n",
        "0  external links\n",
        "5 : List of probability distributions\n",
        "0  external links\n"
       ]
      }
     ],
     "prompt_number": 119
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cursor = db.wikipedia.find({'parse.title':'Wishart distribution'})\n",
      "cursor[0]['parse']['cupcake']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 124,
       "text": [
        "[{u'keywords': None,\n",
        "  u'title': u'Sign In ',\n",
        "  u'url': u'http://biomet.oxfordjournals.org/content/20A/1-2/32'},\n",
        " {u'keywords': None,\n",
        "  u'title': u'zbMATH - the first resource for mathematics',\n",
        "  u'url': u'http://zbmath.org/?format=complete&q=an:54.0565.02'},\n",
        " {u'keywords': None,\n",
        "  u'title': u'Uhlig\\n\\t\\t\\t\\t\\t\\t\\t:\\n\\t\\t\\t\\t\\t\\tOn Singular Wishart and Singular Multivariate Beta Distributions',\n",
        "  u'url': u'http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176325375'},\n",
        " {u'keywords': None,\n",
        "  u'title': u'Peddada\\n\\t\\t\\t\\t\\t\\t\\t,\\n\\t\\t\\t\\t\\t\\tRichards\\n\\t\\t\\t\\t\\t\\t\\t:\\n\\t\\t\\t\\t\\t\\tProof of a Conjecture of M. L. Eaton on the Characteristic Function of the Wishart Distribution',\n",
        "  u'url': u'http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aop/1176990455'},\n",
        " {u'keywords': None,\n",
        "  u'title': u'Invariant generalized functions in homogeneous domains - Springer',\n",
        "  u'url': u'http://link.springer.com/article/10.1007%2FBF01078179'}]"
       ]
      }
     ],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Congratulations!\n",
      "\n",
      "![won-the-internet](http://25.media.tumblr.com/tumblr_m8bg80KH5l1qlh1s6o1_400.gif)\n",
      "\n",
      "#### You have made it to the end (hopefully succcessfully).  Now that you have your data and have contextualized it with information from the web, you can start performing some interesting analyses on it. Some ideas to get you started:\n",
      "\n",
      "* [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_Classification) document clasification using [bag of words](http://en.wikipedia.org/wiki/Bag-of-words_model) vectorization (We will do this next week)\n",
      "* [Six Degrees of Wikipedia](http://en.wikipedia.org/wiki/Wikipedia:Six_degrees_of_Wikipedia) graph traversal \n",
      "* Other awesome things!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}